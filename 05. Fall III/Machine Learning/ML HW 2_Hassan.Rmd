---
title: "ML HW 2"
output: html_document
date: "2022-11-11"
---
```{r}
library(tidyverse)
library(caret)
library(randomForest)
library(xgboost)
library(tidyverse)
library(caret)
library(leaps)
library(glmnet)
library(ggplot2)
library(earth)
library(mgcv)
library(Ckmeans.1d.dp)
library(pROC)
```
## Loading Dataset
```{r}
insurance_t <- read.csv("/Users/fuadhassan/Downloads/Homework2_ML (1)/insurance_t.csv")
```
#Sums of NA's
```{r}
sapply(insurance_t, function(x) sum(is.na(x)))
```
## Imputation Flag
```{r}
insurance_t$ACCTAGE_ind <- as.factor(ifelse(is.na(insurance_t$ACCTAGE), 1, 0))
insurance_t$PHONE_ind <- as.factor(ifelse(is.na(insurance_t$PHONE), 1, 0))
insurance_t$POS_ind <- as.factor(ifelse(is.na(insurance_t$POS), 1, 0))
insurance_t$POSAMT_ind <- as.factor(ifelse(is.na(insurance_t$POSAMT), 1, 0))
insurance_t$INV_ind <- as.factor(ifelse(is.na(insurance_t$INV), 1, 0))
insurance_t$INVBAL_ind <- as.factor(ifelse(is.na(insurance_t$INVBAL), 1, 0))
insurance_t$CC_ind <- as.factor(ifelse(is.na(insurance_t$CC), 1, 0))
insurance_t$CCBAL_ind <- as.factor(ifelse(is.na(insurance_t$CCBAL), 1, 0))
insurance_t$CCPURC_ind <- as.factor(ifelse(is.na(insurance_t$CCPURC), 1, 0))
insurance_t$INCOME_ind <- as.factor(ifelse(is.na(insurance_t$INCOME), 1, 0))
insurance_t$LORES_ind <- as.factor(ifelse(is.na(insurance_t$LORES), 1, 0))
insurance_t$HMVAL_ind <- as.factor(ifelse(is.na(insurance_t$HMVAL), 1, 0))
insurance_t$AGE_ind <- as.factor(ifelse(is.na(insurance_t$AGE), 1, 0))
insurance_t$CRSCORE_ind <- as.factor(ifelse(is.na(insurance_t$CRSCORE), 1, 0))
```

## Changing variables with more than 10 distinct values to numerical and the remaining variables are factors
```{r}
insurance_train <- insurance_t %>%
  mutate_if(~(is.integer(.) & n_distinct(.) > 10), as.numeric)%>%
  mutate_if(~(is.integer(.) & n_distinct(.) < 10), as.factor) %>%
  mutate_if(~(is.character(.) & n_distinct(.) < 10), as.factor) %>%
  mutate_if(~(is.character(.) & n_distinct(.) > 10), as.factor)
```

```{r}
table(insurance_train$INS, insurance_train$MMCRED)
```
## Collapsing MMCRED
```{r}
insurance_train$MMCRED <- fct_collapse(insurance_train$MMCRED, "3+" = c("3","5"))
```
##Imputing numerical with median
```{r}
for (cols in colnames(insurance_train)) {
  if (cols %in% names(insurance_train[,sapply(insurance_train, is.numeric)])) {
    insurance_train<-insurance_train%>%
      mutate(!!cols := replace(!!rlang::sym(cols), is.na(!!rlang::sym(cols)), median(!!rlang::sym(cols), na.rm=TRUE)))
  }}
```
##Imputing categorical with mode
```{r}
getmode <- function(v){
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]}

insurance_train$INV[is.na(insurance_train$INV)] <- getmode(insurance_train$INV)

insurance_train$CC[is.na(insurance_train$CC)] <- getmode(insurance_train$CC)

insurance_train$CCPURC[is.na(insurance_train$CCPURC)] <- getmode(insurance_train$CCPURC)
```
### Viewing if values imputed
```{r}
print(insurance_train$CCPURC)
unique(insurance_train$CCPURC)
```
# Building random forest model
```{r}
set.seed(123)
#insurance_train$random <- rnorm(8495)
insurance_train$INS <- as.factor(insurance_train$INS)
rf.train <- randomForest(INS ~ ., data = insurance_train, type = "classification" , mtry = 7, ntree = 280, importance = TRUE)
```

```{r}
## Checking optimal number of trees - 280
plot(rf.train, main = "Number of Trees Compared to MSE")
```

```{r}
### Variable importance
variables = importance(rf.train,type = 1)
write.csv(variables,"/Users/fuadhassan/Desktop/variables.csv", row.names = TRUE)
```
## Building ROC Curve
```{r}
# Plotting ROC Curve for Random Forest
set.seed(123)
insurance_train$p_hat <- predict(rf.train, type = "prob")[,2]
rf.roc <- roc(insurance_train$INS, insurance_train$p_hat)
# plot(rf.roc, lwd = 3, col = "dodgerblue3", main = paste0("ROC of Random Forest Model(AUC = ", round(auc(rf.roc), 4),")"), 
#      xlab = "True Positive Rate",
#      # ylab = "False Positive Rate")
## ROC Values .793
#auc(rf.roc)
pred1 <- prediction(insurance_train$p_hat, factor(insurance_train$INS))
r.perf <- performance(pred1, measure = "tpr", x.measure = "fpr")
plot(r.perf, lwd = 3, col = "dodgerblue3", main = paste0("ROC of Random Forest Model(AUC = ",sprintf("%.4f",round(auc(rf.roc), 4)),")"), 
     xlab = "True Positive Rate",
     ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
```
### Building XG Boost Model
```{r}
# Prepare data for XGBoost function - similar to what we did for glmnet
insurance_train <- insurance_train %>%
  dplyr::select(-c(p_hat, INVBAL))
ins <- insurance_train
#ins$random <- rnorm(8495)
ins$INS <- as.numeric(ins$INS) - 1
train_x <- model.matrix(INS ~ ., data = ins)[, -1]
train_y <- ins$INS
```
### Building XG Boost Model
```{r}
## Old Model
set.seed(123)
xgb.ins <- xgboost(data = train_x, label = train_y, subsample = 0.5, nrounds = 100, objective = "binary:logistic")
```

```{r}
set.seed(123)
## Tuning for the nrounds = 10
xgb.ins.cv <- xgb.cv(data = train_x, label = train_y, subsample = 0.5, nrounds = 100,objective = "binary:logistic",eval_metric = 'auc', nfold = 10)
```
```{r}
#Changing back to a factor
train_y1 <- as.factor(train_y)
#Tuning through caret
tune_grid <- expand.grid(
  nrounds = 10,
  eta = c(0.1, 0.15, 0.2, 0.25, 0.3),
  max_depth = c(1:10),
  gamma = c(0),
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = c(0.25, 0.5, 0.75, 1)
)

xgb.ins.caret <- train(x = train_x, y = train_y1,
      method = "xgbTree",
      tuneGrid = tune_grid,
      trControl = trainControl(method = 'cv', # Using 10-fold cross-validation
                               number = 10))
#plot(xgb.ins.caret)
```

```{r}
## Grabbing values from chart above
xgb.ins.caret$bestTune
```
```{r}
##New Model
set.seed(123)
xgb.ins1 <- xgboost(data = train_x, label = train_y, subsample = 1, nrounds = 10,max_depth = 4, eta = .25, objective = "binary:logistic")
```

```{r}
## Variable Importance after taking out INVBAL
xg.imp = xgb.importance(feature_names = colnames(train_x), model = xgb.ins1)
xg.imp
#write.csv(xg.imp,"/Users/fuadhassan/Desktop/variables_XG.csv", row.names = TRUE)
```
## Building ROC Curve
```{r}
##XGB ROC CURVE
ins$p_hat <- predict(xgb.ins1, train_x)
pred2 <- prediction(ins$p_hat, factor(ins$INS))
xgb.perf <- performance(pred2, measure = "tpr", x.measure = "fpr")
plot(xgb.perf, lwd = 3, col = "dodgerblue3", main = paste0("ROC of XGBoost Model(AUC = ",sprintf("%.4f",round(auc(xgb.roc), 4)),")"), 
     xlab = "True Positive Rate",
     ylab = "False Positive Rate")
abline(a = 0, b = 1, lty = 3)
## ROC Values .821
auc(xgb.roc)
```
```{r}
### Lift for XGBoost Model
pred.XGB <- prediction(ins$p_hat, factor(ins$INS))
perf.XGB <- performance(pred.XGB, measure = "lift", x.measure = "rpp")
plot(perf.XGB, lwd = 3, colorize = TRUE, colorkey = TRUE,
     colorize.palette = rev(gray.colors(256)),
     main = "Lift Chart for Training Data")
abline(h = 2.2, lty = 3)
abline(v = 0.20, lty = 3)
```










