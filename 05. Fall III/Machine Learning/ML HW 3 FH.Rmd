---
title: "ML HW 3"
output: html_document
date: "2022-11-18"
---
```{r}
# Libraries
library(tidyverse)
library(caret)
library(randomForest)
library(xgboost)
library(tidyverse)
library(caret)
library(leaps)
library(glmnet)
library(ggplot2)
library(earth)
library(mgcv)
library(Ckmeans.1d.dp)
library(pROC)
library(e1071)
library(ggplot2)
library(naivebayes)
```
```{r}
## setting seed
set.seed(478)
```

```{r}
### Importing train and validation dataset
insurance_t <- read.csv("/Users/fuadhassan/Downloads/Homework3_ML (1)/insurance_t.csv")
insurance_v <- read.csv("/Users/fuadhassan/Downloads/Homework3_ML (1)/insurance_v.csv")
```

```{r}
#Sums of NA's
sapply(insurance_t, function(x) sum(is.na(x)))
sapply(insurance_v, function(x) sum(is.na(x)))
```
```{r}
# Imptation flag for train
insurance_t$ACCTAGE_ind <- as.factor(ifelse(is.na(insurance_t$ACCTAGE), 1, 0))
insurance_t$PHONE_ind <- as.factor(ifelse(is.na(insurance_t$PHONE), 1, 0))
insurance_t$POS_ind <- as.factor(ifelse(is.na(insurance_t$POS), 1, 0))
insurance_t$POSAMT_ind <- as.factor(ifelse(is.na(insurance_t$POSAMT), 1, 0))
insurance_t$INV_ind <- as.factor(ifelse(is.na(insurance_t$INV), 1, 0))
insurance_t$INVBAL_ind <- as.factor(ifelse(is.na(insurance_t$INVBAL), 1, 0))
insurance_t$CC_ind <- as.factor(ifelse(is.na(insurance_t$CC), 1, 0))
insurance_t$CCBAL_ind <- as.factor(ifelse(is.na(insurance_t$CCBAL), 1, 0))
insurance_t$CCPURC_ind <- as.factor(ifelse(is.na(insurance_t$CCPURC), 1, 0))
insurance_t$INCOME_ind <- as.factor(ifelse(is.na(insurance_t$INCOME), 1, 0))
insurance_t$LORES_ind <- as.factor(ifelse(is.na(insurance_t$LORES), 1, 0))
insurance_t$HMVAL_ind <- as.factor(ifelse(is.na(insurance_t$HMVAL), 1, 0))
insurance_t$AGE_ind <- as.factor(ifelse(is.na(insurance_t$AGE), 1, 0))
insurance_t$CRSCORE_ind <- as.factor(ifelse(is.na(insurance_t$CRSCORE), 1, 0))

## imputation flag for validation
insurance_v$ACCTAGE_ind <- as.factor(ifelse(is.na(insurance_v$ACCTAGE), 1, 0))
insurance_v$PHONE_ind <- as.factor(ifelse(is.na(insurance_v$PHONE), 1, 0))
insurance_v$POS_ind <- as.factor(ifelse(is.na(insurance_v$POS), 1, 0))
insurance_v$POSAMT_ind <- as.factor(ifelse(is.na(insurance_v$POSAMT), 1, 0))
insurance_v$INV_ind <- as.factor(ifelse(is.na(insurance_v$INV), 1, 0))
insurance_v$INVBAL_ind <- as.factor(ifelse(is.na(insurance_v$INVBAL), 1, 0))
insurance_v$CC_ind <- as.factor(ifelse(is.na(insurance_v$CC), 1, 0))
insurance_v$CCBAL_ind <- as.factor(ifelse(is.na(insurance_v$CCBAL), 1, 0))
insurance_v$CCPURC_ind <- as.factor(ifelse(is.na(insurance_v$CCPURC), 1, 0))
insurance_v$INCOME_ind <- as.factor(ifelse(is.na(insurance_v$INCOME), 1, 0))
insurance_v$LORES_ind <- as.factor(ifelse(is.na(insurance_v$LORES), 1, 0))
insurance_v$HMVAL_ind <- as.factor(ifelse(is.na(insurance_v$HMVAL), 1, 0))
insurance_v$AGE_ind <- as.factor(ifelse(is.na(insurance_v$AGE), 1, 0))
insurance_v$CRSCORE_ind <- as.factor(ifelse(is.na(insurance_v$CRSCORE), 1, 0))
```
## Changing variables with more than 10 distinct values to numerical and the remaining variables are factors
```{r}
#train
insurance_t <- insurance_t %>%
  mutate_if(~(is.integer(.) & n_distinct(.) > 10), as.numeric)%>%
  mutate_if(~(is.integer(.) & n_distinct(.) < 10), as.factor) %>%
  mutate_if(~(is.character(.) & n_distinct(.) < 10), as.factor) %>%
  mutate_if(~(is.character(.) & n_distinct(.) > 10), as.factor)
#validation
insurance_v <- insurance_v %>%
  mutate_if(~(is.integer(.) & n_distinct(.) > 10), as.numeric)%>%
  mutate_if(~(is.integer(.) & n_distinct(.) < 10), as.factor) %>%
  mutate_if(~(is.character(.) & n_distinct(.) < 10), as.factor) %>%
  mutate_if(~(is.character(.) & n_distinct(.) > 10), as.factor)
```

```{r}
###checking for separation
# tnames <- names(insurance_t)
# 
# for (u in tnames){
#   separation<-table(insurance_t$INS, insurance_t[,u]) 
#   print(u)
#   print(separation)
# }
```

```{r}
## Collapsing MMCRED
insurance_t$MMCRED <- fct_collapse(insurance_t$MMCRED, "3" = c("3","5"))
insurance_v$MMCRED <- fct_collapse(insurance_v$MMCRED, "3" = c("3","5"))
```
##Imputing numerical with median
```{r}
#train
for (cols in colnames(insurance_t)) {
  if (cols %in% names(insurance_t[,sapply(insurance_t, is.numeric)])) {
    insurance_t<-insurance_t%>%
      mutate(!!cols := replace(!!rlang::sym(cols), is.na(!!rlang::sym(cols)), median(!!rlang::sym(cols), na.rm=TRUE)))
  }}
#validation
for (cols in colnames(insurance_v)) {
  if (cols %in% names(insurance_v[,sapply(insurance_v, is.numeric)])) {
    insurance_v<-insurance_v%>%
      mutate(!!cols := replace(!!rlang::sym(cols), is.na(!!rlang::sym(cols)), median(!!rlang::sym(cols), na.rm=TRUE)))
  }}
```
##Imputing categorical with mode
```{r}
getmode <- function(v){
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]}
##train
insurance_t$INV[is.na(insurance_t$INV)] <- getmode(insurance_t$INV)

insurance_t$CC[is.na(insurance_t$CC)] <- getmode(insurance_t$CC)

insurance_t$CCPURC[is.na(insurance_t$CCPURC)] <- getmode(insurance_t$CCPURC)

##Validation
insurance_v$INV[is.na(insurance_v$INV)] <- getmode(insurance_v$INV)

insurance_v$CC[is.na(insurance_v$CC)] <- getmode(insurance_v$CC)

insurance_v$CCPURC[is.na(insurance_v$CCPURC)] <- getmode(insurance_v$CCPURC)
```
##Naive Bayes Model
```{r}
# Naive Bayes model
set.seed(854)
nb.t <- naiveBayes(INS ~ ., data = insurance_t, laplace = 0, usekernel = FALSE)
summary(nb.t)
```

```{r}
set.seed(854)
# Optimize laplace and kernel
nb_grid <-   expand.grid(usekernel = c(TRUE, FALSE),
                         laplace = c(0, 0.5, 1),
                         adjust = c(.5, .75, 1, 1.25))
# Fit the Naive Bayes model 
set.seed(2550)
naive_bayes_via_caret2 <- train(INS ~ ., data = insurance_t, 
                               method = "naive_bayes",
                               trControl = trainControl(method = 'cv', number = 10), 
                               tuneGrid = nb_grid)
```

```{r}
## Grabbing tunning parameter
## Kernal = FALSE, laplace = 0
naive_bayes_via_caret2$
```
```{r}
#naive_pred <- Predictor$new(naive_bayes_via_caret2$finalModel, data = insurance_t[,-37], y = insurance_t$INS, type = 'prob')

#plot(FeatureImp$new(naive_pred, loss = 'mse'))
```

##Plotting ROC Curve
```{r}
## Train ROC Curve
set.seed(854)
insurance_t$p_hat <- predict(nb.t,insurance_t, type = 'raw')[,2]
#insurance_t$p_hat <- format(insurance_t$p_hat, scientific = FALSE)
nb.roc <- roc(insurance_t$INS, insurance_t$p_hat)
plot(nb.roc, lwd = 3, col = "dodgerblue3", main = paste0("ROC of Naive Bayes Model(AUC = ",sprintf("%.4f",round(auc(nb.roc), 4)),")"),
     xlab = "True Positive Rate",
      ylab = "False Positive Rate")
## ROC Values .7199
```
```{r}
## Validation ROC Curve
set.seed(854)
p_hat <- predict(nb.t,type = "raw", newdata = insurance_v)[,2]
nb.v.roc <- roc(insurance_v$INS, p_hat)
plot(nb.v.roc, lwd = 3, col = "dodgerblue3", main = paste0("ROC Curve of Naive Model(AUC = ", round(auc(nb.v.roc), 4),")"), 
     xlab = "True Positive Rate",
      ylab = "False Positive Rate")
```
#Final Model Chosen XGBoost
```{r}
set.seed(854)
ins <- insurance_t %>%
  dplyr::select(-c( INVBAL)) ## Removing INV Bal because it performed worse than random
#ins$random <- rnorm(8495)
ins$INS <- as.numeric(ins$INS) - 1
train_x <- model.matrix(INS ~ ., data = ins)[, -1]
train_y <- ins$INS
xgb.ins1 <- xgboost(subsample = 1, nrounds = 10,max_depth = 4, eta = .25, objective = "binary:logistic", data = train_x, label = train_y)
```

```{r}
train_y1 <- as.factor(train_y)
#Tuning through caret
tune_grid <- expand.grid(
  nrounds = 10,
  eta = .25,
  max_depth = 4,
  gamma = c(0),
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb.ins.caret <- caret::train(x = train_x, y = train_y1,
      method = "xgbTree",
      tuneGrid = tune_grid,
      trControl = trainControl(method = 'cv', # Using 10-fold cross-validation
                               number = 10))
```

```{r}
## Validation
ins.v = insurance_v %>%
  select(-c(INVBAL))
ins.v$INS = as.numeric(ins.v$INS) - 1
train_vx <- model.matrix(INS ~ ., data = ins.v)[, -1]
train_vy <- ins.v$INS
```

```{r}
## Variable Importance
xg.imp = xgb.importance(feature_names = colnames(train_x), model = xgb.ins)
xg.imp
write.csv(xg.imp,"/Users/fuadhassan/Desktop/variables_XG.csv", row.names = TRUE)
```

```{r}
### Roc curve on the validation dataset
ins.v$p_hat <- predict(xgb.ins1, train_vx)
xgb.v.roc <- roc(ins.v$INS, ins.v$p_hat)
plot(xgb.v.roc, lwd = 3, col = "dodgerblue3", main = paste0("XGBoost Performance on Validation Set(AUC = ",sprintf("%.4f",round(auc(xgb.v.roc), 4)),")"),
     xlab = "True Positive Rate",
      ylab = "False Positive Rate")
## ROC Values .7919
auc(xgb.v.roc)
```
##Model Interpetation
```{r}
##Libraries
library(Ckmeans.1d.dp)
library(pdp)
library(ALEPlot)
library(lime)
library(iml)
```

```{r}
#train_x1 <- data.frame()

xgb_pred <- Predictor$new(xgb.ins.caret, data = data.frame(train_x) , y = train_y)

#ale_plot <- FeatureEffects$new(xgb_pred)

```

```{r}
## Building ALE Plot
ale_plot <- FeatureEffects$new(xgb_pred, method = "ale")
ale_plot$plot(c("ACCTAGE"))
#pd_plot$plot()
```

```{r}
#set.seed(457)
#pdp::partial(xgb.ins.caret, train = as.data.frame(train_x), pred.var = "ACCTAGE", plot = TRUE)
#pdp::plotPartial(xgb.ins$raw,pred.var = "ACCTAGE" )
```
```{r}
#shap <- Shapley$new(xgb_pred, x.interest = train_x1[732,])
#shap$plot()
s_value <- shap$results %>%
  filter(class == 1 )

s_value <- s_value %>%
  filter(phi.var != 0)
write.csv(s_value,"/Users/fuadhassan/Desktop/shapley.csv", row.names = TRUE)

```






